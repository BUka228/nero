# --- PROJECT CONFIGURATION ---

# 1. FILE PATHS
# Path to the exported Telegram JSON file
RAW_DATA_PATH="data/raw/result.json"

# Folder where clean JSONL and RAG index will be stored
PROCESSED_DATA_DIR="data/processed"

# 2. PARSING SETTINGS
# Time in seconds to split conversations.
# If messages are apart by more than this time, they are treated as separate context.
# 7200 seconds = 2 hours.
CONVERSATION_TIMEOUT=7200

# 3. USER CONFIGURATION
# IDs can be found in result.json ("id" field).
# DO NOT include "user" prefix, just numbers.

# --- FIRST USER (e.g. You) ---
USER_1_ID=123456789
USER_1_NAME="Alice"
# System prompt defines how the model should behave when impersonating this user.
# Describe style, typical slang, length of messages, and personality.
USER_1_SYSTEM_PROMPT="You are Alice. You reply briefly, strictly to the point. You rarely use emojis. You are sarcastic."

# --- SECOND USER (e.g. Your Friend) ---
USER_2_ID=987654321
USER_2_NAME="Bob"
USER_2_SYSTEM_PROMPT="You are Bob. You are very emotional, use lots of stickers and caps lock. You love video games."

# 5. RAG / VECTOR DB SETTINGS
# Path to store ChromaDB files
CHROMA_DB_PATH="data/vector_db"
# Embedding model for retrieval (multilingual recommended for Russian)
EMBEDDING_MODEL="intfloat/multilingual-e5-large"

# --- TRAINING CONFIGURATION (LoRA) ---

# Model ID from HuggingFace (Optimized for Mac)
# Qwen3-4B is excellent, but you can also use Llama-3.2-3B-Instruct
BASE_MODEL_PATH="Qwen/Qwen3-4B-MLX-4bit"

# Batch size:
# 16GB RAM -> 4
# 8GB RAM -> 1 or 2
LORA_BATCH_SIZE=4

# Training iterations (Steps).
# 600 - fast check (might be undertrained)
# 1000 - standard for style copy
# 2000+ - deep learning (risk of overfitting)
LORA_ITERS=1000

# LoRA Rank (Capacity of the adapter).
# 8 - minimal
# 16 - standard balanced
# 32 - smart but heavy
LORA_RANK=8