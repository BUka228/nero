import os
import json
import math
import torch
import chromadb
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from chromadb.config import Settings

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
load_dotenv()

PROCESSED_DIR = Path(os.getenv("PROCESSED_DATA_DIR", "data/processed"))
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "data/vector_db")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

# –†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏. –ù–∞ M4 –º–æ–∂–Ω–æ —Å—Ç–∞–≤–∏—Ç—å –ø–æ–±–æ–ª—å—à–µ (256-512) –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.
BATCH_SIZE = 512 

class VectorIndexer:
    def __init__(self):
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞...")
        
        # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (Apple Silicon Optimization)
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device.upper()}")

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–æ–¥–∏–Ω —Ä–∞–∑ –≤ –ø–∞–º—è—Ç—å)
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL_NAME}...")
        self.encoder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=self.device)

        # 3. –ö–ª–∏–µ–Ω—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        print(f"üíΩ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB: {CHROMA_DB_PATH}")
        self.client = chromadb.PersistentClient(
            path=CHROMA_DB_PATH,
            settings=Settings(anonymized_telemetry=False)
        )

    def _format_passage(self, text: str) -> str:
        if not text:
            return ""
        stripped = text.lstrip()
        if stripped.lower().startswith("passage:"):
            return text
        return f"passage: {text}"

    def _batch_generator(self, data: List[Any], batch_size: int):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        1. –ü–∞—á–∫—É –¥–∞–Ω–Ω—ã—Ö (batch)
        2. –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–∞—á–∞–ª–∞ —ç—Ç–æ–π –ø–∞—á–∫–∏ (start_index)
        """
        for i in range(0, len(data), batch_size):
            yield data[i : i + batch_size], i

    def index_chat_history(self):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø–µ—Ä–µ–ø–∏—Å–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞"""
        input_file = PROCESSED_DIR / "chat_history_rag.json"
        
        if not input_file.exists():
            print(f"‚ö†Ô∏è –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫.")
            return

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        collection_name = "chat_history"
        print(f"\nüìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞—Ç–∞ ({len(data)} —Å–æ–æ–±—â–µ–Ω–∏–π) –≤ '{collection_name}'...")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        try:
            self.client.delete_collection(collection_name)
        except Exception:
            pass
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é (cosine distance –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –¥–ª—è E5 –ø–æ–¥—Ö–æ–¥–∏—Ç)
        collection = self.client.create_collection(name=collection_name)

        # –ü—Ä–æ—Ü–µ—Å—Å –ø–æ –±–∞—Ç—á–∞–º
        total_batches = math.ceil(len(data) / BATCH_SIZE) if data else 0
        
        for batch, start_index in tqdm(self._batch_generator(data, BATCH_SIZE), total=total_batches):
            documents = []
            metadatas = []
            ids = []

            for i, item in enumerate(batch):
                # –í—ã—á–∏—Å–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –Ω–æ–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–ø–∏—Å–∫–µ
                # –≠—Ç–æ –ö–õ–Æ–ß–ï–í–û–ô –º–æ–º–µ–Ω—Ç –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                global_idx = start_index + i
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞: "–†–æ–ª—å: –¢–µ–∫—Å—Ç"
                raw_text = f"{item['role']}: {item['content']}"
                text_content = self._format_passage(raw_text)
                
                documents.append(text_content)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º global_index –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                metadatas.append({
                    "user_id": str(item.get("user_id", "")),
                    "role": item.get("role", "unknown"),
                    "timestamp": int(item.get("timestamp", 0)),
                    "date": item.get("date", ""),
                    "global_index": global_idx  # <--- –í–û–¢ –û–ù–û
                })
                
                # ID –¥–µ–ª–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º (–≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å)
                ids.append(str(global_idx))

            if documents:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã
                embeddings = self.encoder.encode(
                    documents,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    normalize_embeddings=True,
                )
                
                # –ü–∏—à–µ–º –≤ –±–∞–∑—É
                collection.add(
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )

        print(f"‚úÖ –ß–∞—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω.")

    def index_stickers(self):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å—Ç–∏–∫–µ—Ä–æ–≤ (–∑–¥–µ—Å—å –ø–æ—Ä—è–¥–æ–∫ –Ω–µ –≤–∞–∂–µ–Ω, –≤–∞–∂–µ–Ω —Å–º—ã—Å–ª)"""
        input_file = PROCESSED_DIR / "stickers_metadata.json"
        
        if not input_file.exists():
            print(f"‚ö†Ô∏è –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫.")
            return

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        collection_name = "sticker_search"
        print(f"\nüé® –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å—Ç–∏–∫–µ—Ä–æ–≤ ({len(data)} —à—Ç.) –≤ '{collection_name}'...")

        try:
            self.client.delete_collection(collection_name)
        except Exception:
            pass
        
        collection = self.client.create_collection(name=collection_name)

        total_batches = math.ceil(len(data) / BATCH_SIZE) if data else 0

        for batch, _ in tqdm(self._batch_generator(data, BATCH_SIZE), total=total_batches):
            documents = []
            metadatas = []
            ids = []

            for item in batch:
                desc = item.get("description", "")
                if not desc:
                    continue

                text_content = self._format_passage(desc)

                documents.append(text_content)
                metadatas.append({
                    "path": item.get("path", ""),
                    "type": item.get("type", "static")
                })
                ids.append(item.get("path")) # ID = –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

            if documents:
                embeddings = self.encoder.encode(
                    documents,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    normalize_embeddings=True,
                )
                collection.add(
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )

        print(f"‚úÖ –°—Ç–∏–∫–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã.")

if __name__ == "__main__":
    indexer = VectorIndexer()
    indexer.index_chat_history()
    indexer.index_stickers()