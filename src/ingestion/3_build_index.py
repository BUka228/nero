import os
import json
import torch
import chromadb
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from chromadb.config import Settings

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
load_dotenv()

PROCESSED_DIR = Path(os.getenv("PROCESSED_DATA_DIR", "data/processed"))
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "data/vector_db")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–µ–π (—á–µ–º –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏, —Ç–µ–º –±–æ–ª—å—à–µ –º–æ–∂–Ω–æ —Å—Ç–∞–≤–∏—Ç—å)
BATCH_SIZE = 256 

class VectorIndexer:
    def __init__(self):
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorIndexer...")
        
        # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (M4 Optimization)
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.device.upper()}")

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL_NAME}...")
        self.encoder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=self.device)

        # 3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB
        print(f"üíΩ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {CHROMA_DB_PATH}")
        self.client = chromadb.PersistentClient(
            path=CHROMA_DB_PATH,
            settings=Settings(anonymized_telemetry=False)
        )

    def _batch_generator(self, data: List[Any], batch_size: int):
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–∞—á–∫–∏"""
        for i in range(0, len(data), batch_size):
            yield data[i : i + batch_size]

    def index_chat_history(self):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –ø–µ—Ä–µ–ø–∏—Å–∫–∏"""
        input_file = PROCESSED_DIR / "chat_history_rag.json"
        
        if not input_file.exists():
            print(f"‚ö†Ô∏è –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —á–∞—Ç–∞.")
            return

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        collection_name = "chat_history"
        print(f"\nüìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ ({len(data)} –∑–∞–ø–∏—Å–µ–π) –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é '{collection_name}'...")
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω—É–ª—è, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –¥—É–±–ª–µ–π
        try:
            self.client.delete_collection(collection_name)
        except:
            pass
        
        collection = self.client.create_collection(name=collection_name)

        # –ü—Ä–æ—Ü–µ—Å—Å –±–∞—Ç—á–∞–º–∏
        for batch in tqdm(self._batch_generator(data, BATCH_SIZE), total=(len(data) // BATCH_SIZE) + 1):
            documents = []
            metadatas = []
            ids = []

            for idx, item in enumerate(batch):
                # –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                # –î–ª—è –º–æ–¥–µ–ª–∏ E5 —á–∞—Å—Ç–æ –¥–æ–±–∞–≤–ª—è—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å "passage: ", –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã RAG –º–æ–∂–Ω–æ –∏ –±–µ–∑, 
                # –µ—Å–ª–∏ query —Ç–æ–∂–µ –±—É–¥–µ—Ç –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞.
                # –î–ª—è —á–∞—Ç–æ–≤ –≤–∞–∂–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç: –ö—Ç–æ —Å–∫–∞–∑–∞–ª + –ß—Ç–æ —Å–∫–∞–∑–∞–ª.
                text_content = f"{item['role']}: {item['content']}"
                
                documents.append(text_content)
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—á—Ç–æ–±—ã –ø–æ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∞—Ç–µ –∏–ª–∏ –∞–≤—Ç–æ—Ä—É)
                metadatas.append({
                    "user_id": str(item.get("user_id", "")),
                    "role": item.get("role", "unknown"),
                    "timestamp": int(item.get("timestamp", 0)),
                    "date": item.get("date", "")
                })
                
                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID (–º–æ–∂–Ω–æ timestamp + user_id, –Ω–æ –ø—Ä–æ—â–µ uuid –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π)
                # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º timestamp –∫–∞–∫ —á–∞—Å—Ç—å ID –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞
                ids.append(f"{item.get('timestamp')}_{idx}")

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embeddings = self.encoder.encode(documents, convert_to_numpy=True, show_progress_bar=False)
            
            # –ó–∞–ø–∏—Å—å –≤ –±–∞–∑—É
            collection.add(
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas,
                ids=ids
            )

        print(f"‚úÖ –ß–∞—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω.")

    def index_stickers(self):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π —Å—Ç–∏–∫–µ—Ä–æ–≤"""
        input_file = PROCESSED_DIR / "stickers_metadata.json"
        
        if not input_file.exists():
            print(f"‚ö†Ô∏è –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å—Ç–∏–∫–µ—Ä–æ–≤.")
            return

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        collection_name = "sticker_search"
        print(f"\nüé® –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å—Ç–∏–∫–µ—Ä–æ–≤ ({len(data)} –∑–∞–ø–∏—Å–µ–π) –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é '{collection_name}'...")

        try:
            self.client.delete_collection(collection_name)
        except:
            pass
        
        collection = self.client.create_collection(name=collection_name)

        for batch in tqdm(self._batch_generator(data, BATCH_SIZE), total=(len(data) // BATCH_SIZE) + 1):
            documents = [] # –û–ø–∏—Å–∞–Ω–∏—è (—Ç–æ, —á—Ç–æ –∏—â–µ–º)
            metadatas = [] # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (—Ç–æ, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º)
            ids = []

            for idx, item in enumerate(batch):
                desc = item.get("description", "")
                if not desc:
                    continue

                documents.append(desc)
                
                metadatas.append({
                    "path": item.get("path", ""),
                    "type": item.get("type", "static")
                })
                
                # ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
                ids.append(str(item.get("path")))

            if not documents:
                continue

            embeddings = self.encoder.encode(documents, convert_to_numpy=True, show_progress_bar=False)
            
            collection.add(
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas,
                ids=ids
            )

        print(f"‚úÖ –°—Ç–∏–∫–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã.")

if __name__ == "__main__":
    indexer = VectorIndexer()
    indexer.index_chat_history()
    indexer.index_stickers()