import os
import sys
import json
import shutil
import subprocess
import random
from pathlib import Path
from dotenv import load_dotenv

# --- –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ---
load_dotenv()

# –ü—É—Ç–∏
PROCESSED_DIR = Path(os.getenv("PROCESSED_DATA_DIR", "data/processed"))
MODELS_DIR = Path("models")
ADAPTERS_DIR = MODELS_DIR / "adapters"
BASE_MODEL_ID = os.getenv("BASE_MODEL_PATH", "Qwen/Qwen3-4B-MLX-4bit")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA
TRAIN_BATCH_SIZE = int(os.getenv("LORA_BATCH_SIZE", 4)) 
TRAIN_ITERS = int(os.getenv("LORA_ITERS", 1000))
LORA_RANK = int(os.getenv("LORA_RANK", 16))

class LoraTrainer:
    def __init__(self):
        self.user_1_id = os.getenv("USER_1_ID")
        self.user_1_name = os.getenv("USER_1_NAME", "User1")
        self.user_2_id = os.getenv("USER_2_ID")
        self.user_2_name = os.getenv("USER_2_NAME", "User2")

    def _prepare_data_for_mlx(self, source_file: Path, target_dir: Path):
        """–ì–æ—Ç–æ–≤–∏—Ç train.jsonl –∏ valid.jsonl"""
        print(f"‚úÇÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_file.name}...")
        
        if target_dir.exists():
            shutil.rmtree(target_dir)
        target_dir.mkdir(parents=True, exist_ok=True)

        with open(source_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        random.shuffle(lines)
        
        # –°–ø–ª–∏—Ç 95/5
        split_idx = int(len(lines) * 0.95)
        train_data = lines[:split_idx]
        valid_data = lines[split_idx:]
        
        if len(valid_data) == 0 and len(train_data) > 1:
            valid_data = [train_data.pop()]

        with open(target_dir / "train.jsonl", 'w', encoding='utf-8') as f:
            f.writelines(train_data)
        
        with open(target_dir / "valid.jsonl", 'w', encoding='utf-8') as f:
            f.writelines(valid_data)
            
        print(f"üìä Train: {len(train_data)} —Å—Ç—Ä–æ–∫ | Valid: {len(valid_data)} —Å—Ç—Ä–æ–∫")
        return target_dir

    def _create_lora_config(self, config_path: Path):
        """
        –°–æ–∑–¥–∞–µ—Ç YAML –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è LoRA.
        –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–æ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (Flat Structure), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å KeyError: 'rank'.
        """
        config_content = f"""
# LoRA Configuration (Flat Structure for modern mlx-lm)
rank: {LORA_RANK}
alpha: {LORA_RANK * 2}
dropout: 0.05
keys: 
  - "self_attn.q_proj"
  - "self_attn.v_proj"
  - "self_attn.k_proj"
  - "self_attn.o_proj"
  - "mlp.gate_proj"
  - "mlp.down_proj"
  - "mlp.up_proj"
"""
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content.strip())
        print(f"‚öôÔ∏è –°–æ–∑–¥–∞–Ω –∫–æ–Ω—Ñ–∏–≥ LoRA: {config_path}")

    def run_training(self, user_name: str):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ CLI MLX"""
        
        source_jsonl = PROCESSED_DIR / f"train_{user_name}.jsonl"
        
        if not source_jsonl.exists():
            print(f"‚ùå –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {source_jsonl}")
            return

        # 1. –ì–æ—Ç–æ–≤–∏–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
        temp_data_path = PROCESSED_DIR / f"temp_mlx_{user_name}"
        self._prepare_data_for_mlx(source_jsonl, temp_data_path)

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª –¥–ª—è LoRA
        config_path = temp_data_path / "lora_config.yaml"
        self._create_lora_config(config_path)

        # 3. –ü—É—Ç—å –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
        adapter_output_path = ADAPTERS_DIR / user_name
        
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è LoRA –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_name}")
        print(f"ü§ñ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {BASE_MODEL_ID}")
        print("-" * 50)

        # –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞ —Å —Ñ–ª–∞–≥–æ–º -c (config)
        command = [
            "mlx_lm.lora",
            "--model", BASE_MODEL_ID,
            "--train",
            "--data", str(temp_data_path),
            "--adapter-path", str(adapter_output_path),
            "--batch-size", str(TRAIN_BATCH_SIZE),
            "--iters", str(TRAIN_ITERS),
            "--save-every", "100",
            "--config", str(config_path),
            "--grad-checkpoint",
            "--seed", "42"
        ]

        try:
            subprocess.run(command, check=True)
            print("-" * 50)
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ê–¥–∞–ø—Ç–µ—Ä—ã: {adapter_output_path}")
        except subprocess.CalledProcessError as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ (–∫–æ–¥ {e.returncode}).")
        except FileNotFoundError:
             print("\n‚ùå –ö–æ–º–∞–Ω–¥–∞ 'mlx_lm.lora' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")

    def interactive_menu(self):
        print("\n--- LoRA Training Studio (MLX) ---")
        print(f"1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –±—ã—Ç—å: {self.user_1_name}")
        print(f"2. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –±—ã—Ç—å: {self.user_2_name}")
        print("0. –í—ã—Ö–æ–¥")
        
        choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç (1/2): ").strip()
        
        if choice == "1":
            self.run_training(self.user_1_name)
        elif choice == "2":
            self.run_training(self.user_2_name)
        elif choice == "0":
            sys.exit(0)
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä.")

if __name__ == "__main__":
    trainer = LoraTrainer()
    trainer.interactive_menu()