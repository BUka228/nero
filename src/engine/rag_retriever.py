import os
import json
import torch
import chromadb
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from dateparser.search import search_dates

load_dotenv()

PROCESSED_DIR = Path(os.getenv("PROCESSED_DATA_DIR", "data/processed"))
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "data/vector_db")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SEARCH_LIMIT = 30           
MERGE_GAP_THRESHOLD = 20    
PADDING = 5                 
STICKER_THRESHOLD = 0.6
# –ù–û–í–û–ï: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤—Ä–µ–º–µ–Ω–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
# 43200 —Å–µ–∫—É–Ω–¥ = 12 —á–∞—Å–æ–≤. –ï—Å–ª–∏ —Ä–∞–∑—Ä—ã–≤ –±–æ–ª—å—à–µ, –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è.
MAX_TIME_GAP = 43200 

class RagRetriever:
    def __init__(self):
        print("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SMART RAG –¥–≤–∏–∂–∫–∞...")
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        
        self.encoder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=self.device)
        self.client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        
        try:
            self.chat_collection = self.client.get_collection("chat_history")
            self.sticker_collection = self.client.get_collection("sticker_search")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {e}")
            raise

        history_path = PROCESSED_DIR / "chat_history_rag.json"
        with open(history_path, 'r', encoding='utf-8') as f:
            self.full_history = json.load(f)
        
        self.history_len = len(self.full_history)
        print(f"‚úÖ –ò—Å—Ç–æ—Ä–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.history_len} —Å–æ–æ–±—â–µ–Ω–∏–π.")

    def _extract_date_bounds(self, query: str) -> Optional[Tuple[float, float]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç start_ts –∏ end_ts –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞"""
        dates = search_dates(query, languages=['ru', 'en'])
        if not dates: return None
        
        found_text, date_obj = dates[0]
        start_of_day = date_obj.replace(hour=0, minute=0, second=0, microsecond=0).timestamp()
        end_of_day = date_obj.replace(hour=23, minute=59, second=59, microsecond=999999).timestamp()
        
        print(f"üìÖ –†–µ–∂–∏–º –¥–∞—Ç—ã: {date_obj.date()}")
        return (start_of_day, end_of_day)

    def search_context(self, query: str) -> str:
        if not query: return ""

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã
        date_bounds = self._extract_date_bounds(query)
        date_filter = None
        
        if date_bounds:
            start_ts, end_ts = date_bounds
            date_filter = {
                "$and": [
                    {"timestamp": {"$gte": start_ts}},
                    {"timestamp": {"$lte": end_ts}}
                ]
            }

        # 2. –ü–æ–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        query_vec = self.encoder.encode([query], convert_to_numpy=True).tolist()
        
        try:
            results = self.chat_collection.query(
                query_embeddings=query_vec,
                n_results=SEARCH_LIMIT,
                where=date_filter # –§–∏–ª—å—Ç—Ä –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            )
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""

        hit_indices = set()
        if results['metadatas'] and results['metadatas'][0]:
            for meta in results['metadatas'][0]:
                idx = meta.get('global_index')
                if idx is not None: hit_indices.add(idx)

        if not hit_indices: return ""

        # 3. –°–∫–ª–µ–π–∫–∞ –æ—Å—Ç—Ä–æ–≤–æ–≤
        sorted_hits = sorted(list(hit_indices))
        merged_blocks = []
        
        if sorted_hits:
            current_start = max(0, sorted_hits[0] - PADDING)
            current_end = min(self.history_len, sorted_hits[0] + PADDING)

            for i in range(1, len(sorted_hits)):
                next_hit = sorted_hits[i]
                if next_hit <= (current_end + MERGE_GAP_THRESHOLD):
                    new_end = min(self.history_len, next_hit + PADDING)
                    current_end = max(current_end, new_end)
                else:
                    merged_blocks.append((current_start, current_end))
                    current_start = max(0, next_hit - PADDING)
                    current_end = min(self.history_len, next_hit + PADDING)
            merged_blocks.append((current_start, current_end))

        # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –£–ú–ù–û–ô –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô
        final_output = []
        
        for start, end in merged_blocks:
            # –°—Ä–µ–∑–∞–µ–º –±–ª–æ–∫
            safe_end = min(end + 1, self.history_len)
            chunk_indices = range(start, safe_end)
            
            valid_msgs = []
            last_ts = 0

            for idx in chunk_indices:
                msg = self.full_history[idx]
                curr_ts = int(msg.get('timestamp', 0))

                # --- –ü–†–û–í–ï–†–ö–ê 1: –î–ê–¢–ê (–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –¥–∞—Ç—ã) ---
                if date_bounds:
                    # –ï—Å–ª–∏ –º—ã –∏—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É, –ú–´ –ù–ï –î–û–õ–ñ–ù–´ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –¥—Ä—É–≥–∏—Ö –¥–Ω–µ–π,
                    # –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ "–∫–æ–Ω—Ç–µ–∫—Å—Ç".
                    if not (date_bounds[0] <= curr_ts <= date_bounds[1]):
                        continue

                # --- –ü–†–û–í–ï–†–ö–ê 2: –í–†–ï–ú–ï–ù–ù–û–ô –†–ê–ó–†–´–í (Time Gap) ---
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –±–ª–æ–∫–µ –∏ —Ä–∞–∑—Ä—ã–≤ –±–æ–ª—å—à–µ 12 —á–∞—Å–æ–≤
                if last_ts > 0 and (curr_ts - last_ts) > MAX_TIME_GAP:
                    # –ï—Å–ª–∏ —Ä–∞–∑—Ä—ã–≤ —Å–ª—É—á–∏–ª—Å—è –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ —Å–∫–ª–µ–π–∫–∏ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                    if valid_msgs:
                        valid_msgs.append("--- [–ü–†–û–®–õ–û –ú–ù–û–ì–û –í–†–ï–ú–ï–ù–ò] ---")
                
                last_ts = curr_ts
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                try:
                    dt = datetime.fromtimestamp(curr_ts)
                    date_pretty = dt.strftime("%Y-%m-%d %H:%M")
                except:
                    date_pretty = "Unknown"

                role = msg.get('role', 'Unknown')
                text = msg.get('content', '').replace('\n', ' ')
                valid_msgs.append(f"[{date_pretty}] {role}: {text}")

            if valid_msgs:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –±–ª–æ–∫–∏ (–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –∏–∑-–∑–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –¥–∞—Ç—ã)
                cleaned_block = "\n".join(valid_msgs)
                if "[" in cleaned_block: # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –µ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è
                    final_output.append(cleaned_block)

        return "\n\n--- [–ù–û–í–´–ô –î–ò–ê–õ–û–ì] ---\n\n".join(final_output)

    def find_best_sticker(self, query_text: str) -> Optional[Dict[str, str]]:
        if not query_text or len(query_text) < 2: return None
        query_vec = self.encoder.encode([query_text], convert_to_numpy=True).tolist()
        results = self.sticker_collection.query(query_embeddings=query_vec, n_results=1)
        if not results['ids'] or not results['ids'][0]: return None
        if results['distances'][0][0] > STICKER_THRESHOLD: return None
        meta = results['metadatas'][0][0]
        return {"path": meta.get('path'), "type": meta.get('type')}

if __name__ == "__main__":
    retriever = RagRetriever()
    print("\n--- TEST ---")
    ctx = retriever.search_context("–ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –º—ã —Å–º–æ—Ç—Ä–µ–ª–∏ –¥–æ—Ä–∞–º—É?")
    if ctx:
        print(ctx)
    else:
        print("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")